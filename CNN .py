# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19dDCswazKN8yUyeMtYBg67no2NbxFV0d
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import MNIST
from torch.utils.data import TensorDataset, random_split
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Set random seed for reproducibility
torch.manual_seed(42)

# Define data transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Upload the MNIST files
#from google.colab import files
#uploaded = files.upload()
from google.colab import drive
import numpy as np

drive.mount('/content/drive')

import os

# Specify the file paths for training and testing
train_imgs_pth = '/content/drive/MyDrive/DL_Assign1/train-images.idx3-ubyte'
train_lbls_pth = '/content/drive/MyDrive/DL_Assign1/train-labels.idx1-ubyte'
test_imgs_pth = '/content/drive/MyDrive/DL_Assign1/t10k-images.idx3-ubyte'
test_lbls_pth = '/content/drive/MyDrive/DL_Assign1/t10k-labels.idx1-ubyte'

# Load data from the uploaded files
train_dta = torch.from_numpy(np.fromfile(train_imgs_pth, dtype=np.uint8)[16:].reshape(-1, 28, 28))
train_trgts = torch.from_numpy(np.fromfile(train_lbls_pth, dtype=np.uint8)[8:])

test_dta = torch.from_numpy(np.fromfile(test_imgs_pth, dtype=np.uint8)[16:].reshape(-1, 28, 28))
test_trgts = torch.from_numpy(np.fromfile(test_lbls_pth, dtype=np.uint8)[8:])

# Check if the loaded data has been loaded correctly
print(f"Train Data Shape: {train_dta.shape}, Train Targets Shape: {train_trgts.shape}")
print(f"Test Data Shape: {test_dta.shape}, Test Targets Shape: {test_trgts.shape}")

# Set image dimensions
img_rows, img_cols = 28, 28

# Adjust the data for PyTorch format
if torch.cuda.is_available():
    inpx = (1, img_rows, img_cols)
else:
    inpx = (img_rows, img_cols, 1)

# Create a copy of the data for normalization
x_train = train_dta.clone()
x_test = test_dta.clone()

# Normalize the data
x_train = x_train.float() / 255
x_test = x_test.float() / 255

y_train = train_trgts.long()
y_test = test_trgts.long()

# Part 2 - Building the CNN
def build_and_train_cnn(train_loader, test_loader, n_classes, use_batch_norm=True):
    # Define the CNN model with ReLU activation and softmax output
    class CustomCNN(nn.Module):
        def __init__(self, n_classes, use_batch_norm):
            super(CustomCNN, self).__init__()
            self.conv_nn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=7, stride=1, padding=3)
            self.relu1 = nn.ReLU()
            self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)
            self.conv_nn2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=5, stride=1, padding=2)
            self.relu2 = nn.ReLU()
            self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)
            self.conv_nn3 = nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, stride=2, padding=1)
            self.relu3 = nn.ReLU()
            self.avgpool3 = nn.AvgPool2d(kernel_size=2, stride=2)
            self.fc = nn.Linear(4 * 2 * 2, n_classes)
            self.softmax = nn.Softmax(dim=1)

            if use_batch_norm:
                self.btch_norm1 = nn.BatchNorm2d(16)  # Batch normalization after the first convolution
                self.btch_norm2 = nn.BatchNorm2d(8)  # Batch normalization after the second convolution
                self.btch_norm3 = nn.BatchNorm2d(4)  # Batch normalization after the third convolution

        def forward(self, x):
            x = self.conv_nn1(x)

            if hasattr(self, 'btch_norm1'):
                x = self.btch_norm1(x)

            x = self.relu1(x)
            x = self.maxpool1(x)
            x = self.conv_nn2(x)

            if hasattr(self, 'btch_norm2'):
                x = self.btch_norm2(x)

            x = self.relu2(x)
            x = self.maxpool2(x)
            x = self.conv_nn3(x)

            if hasattr(self, 'btch_norm3'):
                x = self.btch_norm3(x)

            x = self.relu3(x)
            x = self.avgpool3(x)
            x = x.view(-1, 4 * 2 * 2)
            x = self.fc(x)
            x = self.softmax(x)
            return x

    # Instantiate the model, define loss function, and optimizer
    cnn = CustomCNN(n_classes, use_batch_norm)

    totl_parms = sum(p.numel() for p in cnn.parameters())
    trainable_parms = sum(p.numel() for p in cnn.parameters() if p.requires_grad)
    non_trainable_parms = totl_parms - trainable_parms
    print(f'Total parameters: {totl_parms}')
    print(f'Trainable parameters: {trainable_parms}')
    print(f'Non-trainable parameters: {non_trainable_parms}')

    criterion = nn.CrossEntropyLoss()
    optimizr = optim.Adam(cnn.parameters(), lr=0.001)

    # Part 3 - Training the CNN
    n_epochs = 10
    losses = []  # List to store losses for plotting
    accuracies = []  # List to store accuracies for plotting

    for epoch in range(n_epochs):
        running_loss = 0.0
        correct = 0
        totl = 0

        for i, (images, labels) in enumerate(train_loader):
            optimizr.zero_grad()
            outputs = cnn(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizr.step()

            # Accumulate running loss
            running_loss += loss.item()

            # Calculate accuracy
            _, predicted = torch.max(outputs.data, 1)
            totl += labels.size(0)
            correct += (predicted == labels).sum().item()

        # Calculate average loss and accuracy for the epoch
        epoch_loss = running_loss / len(train_loader)
        epoch_accuracy = correct / totl

        # Append to the lists for plotting
        losses.append(epoch_loss)
        accuracies.append(epoch_accuracy)

        # Print epoch statistics
        print(f'Epoch {epoch + 1}/{n_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy * 100:.2f}%')

    # Plotting accuracy and loss per epoch
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(range(1, n_epochs + 1), losses, label='Training Loss', color='red', marker='o')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.xticks(range(1, n_epochs + 1))

    plt.subplot(1, 2, 2)
    plt.plot(range(1, n_epochs + 1), accuracies, label='Training Accuracy', color='green', marker='o')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Adjust horizontal space between subplots
    plt.subplots_adjust(wspace=0.5)

    # Set x-axis ticks to start from 1 with a step of 1
    plt.xticks(range(1, n_epochs + 1))

    plt.show()

    # Testing the CNN
    correct = 0
    totl = 0
    predicted_labels = []
    true_labels = []

    with torch.no_grad():
        for images, labels in test_loader:
            outputs = cnn(images)
            _, predicted = torch.max(outputs.data, 1)

            totl += labels.size(0)
            correct += (predicted == labels).sum().item()

            predicted_labels.extend(predicted.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())

    # Calculate accuracy
    accuracy = 100 * correct / totl
    print('Accuracy on the test set: {:.2f}%'.format(accuracy))

    # Plot confusion matrix
    class_names = [str(i) for i in range(n_classes)]
    cm = confusion_matrix(true_labels, predicted_labels, labels=list(range(n_classes)))

    # Plot confusion matrix using seaborn
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Greens", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()


    return losses, accuracies

# Experiment 1
# Combine data and targets into a TensorDataset
train_dtaset = TensorDataset(x_train.unsqueeze(1), y_train)
test_dtaset = TensorDataset(x_test.unsqueeze(1), y_test)

# Create data loaders
btch_size = 16
train_loadr = DataLoader(train_dtaset, batch_size=btch_size, shuffle=True)
test_loadr = DataLoader(test_dtaset, batch_size=btch_size, shuffle=False)

# Call the function for Experiment 1 without batch normalization
losses_exp1, accuracies_exp1 = build_and_train_cnn(train_loadr, test_loadr, n_classes=10, use_batch_norm=False)

# Call the function for Experiment 1 with batch normalization
losses_exp1_bn, accuracies_exp1_bn = build_and_train_cnn(train_loadr, test_loadr, n_classes=10, use_batch_norm=True)

# Experiment 2
# Mapping of original class indices to new class indices
class_mapping = {0: 0, 6: 0, 1: 1, 7: 1, 2: 2, 3: 2, 8: 2, 5: 2, 4: 3, 9: 3}

btch_size = 16
# Update labels in the training set
y_train_mappd = torch.tensor([class_mapping[label.item()] for label in y_train])
train_dtaset_mappd = TensorDataset(x_train.unsqueeze(1), y_train_mappd)
train_loader_mappd = DataLoader(train_dtaset_mappd, batch_size=btch_size, shuffle=True)

# Update labels in the test set
y_test_mappd = torch.tensor([class_mapping[label.item()] for label in y_test])
test_dtaset_mappd = TensorDataset(x_test.unsqueeze(1), y_test_mappd)
test_loader_mappd = DataLoader(test_dtaset_mappd, batch_size=btch_size, shuffle=False)

# Call the function for Experiment 2 without batch normalization
losses_exp2, accuracies_exp2 = build_and_train_cnn(train_loader_mappd, test_loader_mappd, n_classes=4, use_batch_norm=False)

# Call the function for Experiment 2 with batch normalization
losses_exp2_bn, accuracies_exp2_bn = build_and_train_cnn(train_loader_mappd, test_loader_mappd, n_classes=4, use_batch_norm=True)

# Comparison plotting accuracy and loss per epoch for experiment1 with and without batch normalization.
n_epochs = 10
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(range(1, n_epochs + 1), losses_exp1, label='Exp 1 - Training Loss without batch normalization', color='red', marker='o')
plt.plot(range(1, n_epochs + 1), losses_exp1_bn, label='Exp 1 - Training Loss with batch normalization', color='blue', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right', bbox_to_anchor=(1.0, 1.15))  # Adjust the legend position
plt.xticks(range(1, n_epochs + 1))

plt.subplot(1, 2, 2)
plt.plot(range(1, n_epochs + 1), accuracies_exp1, label='Exp 1 - Training Accuracy without batch normalization', color='green', marker='o')
plt.plot(range(1, n_epochs + 1), accuracies_exp1_bn, label='Exp 1 - Training Accuracy with batch normalization', color='purple', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='upper right', bbox_to_anchor=(1.0, 1.15))  # Adjust the legend position

# Adjust horizontal space between subplots
plt.subplots_adjust(wspace=0.5)

# Set x-axis ticks to start from 1 with a step of 1
plt.xticks(range(1, n_epochs + 1))

plt.show()

# Comparison plotting accuracy and loss per epoch for experiment2 with and without batch normalization.
n_epochs = 10
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(range(1, n_epochs + 1), losses_exp2, label='Exp 2 - Training Loss without batch normalization', color='red', marker='o')
plt.plot(range(1, n_epochs + 1), losses_exp2_bn, label='Exp 2 - Training Loss with batch normalization', color='blue', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right', bbox_to_anchor=(1.0, 1.15))  # Adjust the legend position
plt.xticks(range(1, n_epochs + 1))

plt.subplot(1, 2, 2)
plt.plot(range(1, n_epochs + 1), accuracies_exp2, label='Exp 2 - Training Accuracy without batch normalization', color='green', marker='o')
plt.plot(range(1, n_epochs + 1), accuracies_exp2_bn, label='Exp 2 - Training Accuracy with batch normalization', color='purple', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='upper right', bbox_to_anchor=(1.0, 1.15))  # Adjust the legend position

# Adjust horizontal space between subplots
plt.subplots_adjust(wspace=0.5)

# Set x-axis ticks to start from 1 with a step of 1
plt.xticks(range(1, n_epochs + 1))

plt.show()